{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb582d21-53ae-41d6-a5fb-2e2040f6baa3",
   "metadata": {},
   "source": [
    "# Choice of Model\n",
    "## choosing what to model\n",
    "we are going to model UK term structure of interest rates and the raw dataset will be the time-series of spot yields available from the BoE website\n",
    "\n",
    "decide whether:\n",
    "- model the whole interest rate curve\n",
    "  or\n",
    "- changes in interest rate curve from one period to the next\n",
    "we go with _\"changes in the interest rate curve\"_ since this is more appropriate for the use case of calculating an SCR.\n",
    "\n",
    "- ?? why do we do y_t+1 / y_t and not change / y_t  ??\n",
    "- do we base analysis on covariance matrix or correlation matrix\n",
    "  - and how does this decision affect backing out to arrive at stress\n",
    "\n",
    "### which measure of change in interest rates\n",
    "There are several options:\n",
    "- absolute differences  $ \\Delta y_t = y_t - y_{t-1} $\n",
    "- log differences $ \\Delta y_t = \\log(y_t) - \\log(y_{t-1}) $\n",
    "- percentage change $ \\Delta y_t = \\frac{y_t - y_{t-1}}{y_{t-1}} $\n",
    "\n",
    "abosolute differences don't work well when rates are low. percentage changes can be extremely high when rates are close to zero.  although it is less intuitive log differences ensures percentage changes are comparable across different rate environments.\n",
    "\n",
    "## transforming dataset via principle component analysis \n",
    "To create a parsimonious model for demonstrating the Bayesian technique, we will perform PCA on the raw dataset. This helps reduce noise and extract the most significant patterns of variation. Additionally, it requires fewer parameters than more complex affine term structure models.\n",
    "\n",
    "## Covariance Vs Correlation\n",
    "\n",
    "|Approach   |when to use   |scale |Impact of Variances|\n",
    "|---|---|---|---|\n",
    "|Covariance   | yields at each maturity similar units and variances  |retains original scale of data   | ??magnitude at different maturities has impact <br>|\n",
    "|Correlation   |variances differ by maturity   |standardises data| different volatilities at diffnt maturities doesn't dominate results|\n",
    "\n",
    "?? what are the advantages of using covariances then  ??\n",
    "\n",
    "!!we should probably make a choice with a scatterplot of adjusted dataset!!\n",
    "\n",
    "read here that the two would yield the same results:  https://medium.com/towards-data-science/applying-pca-to-the-yield-curve-4d2023e555b3\n",
    "\n",
    "## Modeling Steps\n",
    "- demeaning the dataset (PCA expects centering around zero)\n",
    "- generate ??covariance or correlation?? matrix\n",
    "- ??eigenvectors and eigenvalues??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812fd03-3a76-47cb-858c-e2567251f230",
   "metadata": {},
   "source": [
    "# Steps\n",
    "- calculate log of change (don't use df.pcnt_  whatever it is)\n",
    "- de mean the dataset\n",
    "- calculate covariance matrix, eigenvalues and eigenvectors\n",
    "- derive a calibration dataset\n",
    "- attempt to fit normal or student-t distribution  (student t better for heavier tails)\n",
    "  - Q Q plot or historgrams  \n",
    "  - q q plot great for seeing if normally distributed\n",
    "- ?? BAYESIAN INFERENCE PARTS OF THE PROCESS ??\n",
    "  - any hyperparameters\n",
    "- ?? DERIVING STRESSES, COMPARING CLASSICAL VS BAYESIAN APPROACH ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cde6e6-2f6f-4065-8a91-0407fe03bb56",
   "metadata": {},
   "source": [
    "## Deriving Stresses\n",
    "$Y_t = \\log {\\frac{X_t}{X_{t-1}}}$ <br><br>\n",
    "simulate $Y_t$<br><br>\n",
    "$ \\exp{Y_t} = \\frac{X_t}{X_{t-1}} $<br><br>\n",
    "$X_{t-1}e^{Y_t}  = X_t $<br><br>\n",
    "$X_t-1$ is current value of the curve and $X_t$ value one year from now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816043a6-28ab-494f-9629-bb161087f1eb",
   "metadata": {},
   "source": [
    "### the steps\n",
    "- draw realisation of PC from probablistic model\n",
    "- (if using correlations) rescale using s.d. for each yield maturity\n",
    "- add back the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d34c17-0a1d-44f7-baff-b7e016aacfbe",
   "metadata": {},
   "source": [
    "# Getting Raw Data into Dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ea870-875d-4a67-a720-3d67332b9dbb",
   "metadata": {},
   "source": [
    "The bank of england provides two spreadsheets with historic spot yields at https://www.bankofengland.co.uk/statistics/yield-curves/\n",
    "we import each of these into a dataframes (df1 and df2) and join to make a single dataframe (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "71e1ae0a-efdf-4d40-a09f-667ef726aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load in first spreadsheet to df1\n",
    "df1 = pd.read_excel(\"GLC Nominal month end data_1970 to 2015.xlsx\",sheet_name=\"4. spot curve\",engine=\"openpyxl\",skiprows=5,header=None)\n",
    "#create an appropriate set of headers\n",
    "col_names=pd.read_excel(\"GLC Nominal month end data_1970 to 2015.xlsx\",sheet_name=\"4. spot curve\",engine=\"openpyxl\",skiprows=3,nrows=1,header=None)\n",
    "df1.columns = col_names.iloc[0] \n",
    "col_names[0]=\"Date\"\n",
    "#load in second spreadsheet to df2\n",
    "df2 = pd.read_excel(\"GLC Nominal month end data_2016 to present.xlsx\",sheet_name=\"4. spot curve\",engine=\"openpyxl\",skiprows=5,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f8b71548-e171-46cb-9e8b-58c60dedfb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first dates is 1970-01-31 and the last is 2015-12-31\n",
      "one would therefore expect 12 x 46yrs = 552 entries\n",
      "and indeed we see the number of rows in df is 552\n"
     ]
    }
   ],
   "source": [
    "#join the two dataframes to create df\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "#producing some sense checks\n",
    "print(\"the first dates is \"+ str(df.iloc[0,0].strftime('%Y-%m-%d'))+\" and the last is \" +str(df.iloc[551,0].strftime('%Y-%m-%d') ))\n",
    "print(\"one would therefore expect 12 x 46yrs = 552 entries\")\n",
    "print(\"and indeed we see the number of rows in df is \"+str(len(df1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376cba0-ae01-4ce8-bb01-4a9f1c011aa3",
   "metadata": {},
   "source": [
    "# Incorporating Bayesian Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ebb68-ebb5-44ae-9808-c3915952c71d",
   "metadata": {},
   "source": [
    "PCA reveals latent factors ("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4776caf7-7a92-4af3-9854-2eaf2c519592",
   "metadata": {},
   "source": [
    "Having decided to model interest principle components, which economic outlooks correspond to these components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4c9f9-5055-4f79-a09a-3886ae7fb888",
   "metadata": {},
   "source": [
    "|Principle Component   |Relevant Insights   |\n",
    "|---|---|\n",
    "|PC1|level of interest rates  -  expected prolonged rates or gradual hiking against prolonged inflation |\n",
    "|PC2|slope  -  short term vs long term expectations   |\n",
    "|PC3|curvature  -   short term vs long term expectations   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8125ea-d40e-48b7-bfaf-b6cba5713fba",
   "metadata": {},
   "source": [
    "coud the bayesian rules enforce arbitrage freeness ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a581f126-e9e0-45d7-826b-2ce34d5cb8ae",
   "metadata": {},
   "source": [
    "- Economic theory imposes contraints of the first moments (see https://www.nber.org/system/files/working_papers/w24618/w24618.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91d687-1c22-4929-b884-25280427ca1a",
   "metadata": {},
   "source": [
    "# Some links\n",
    "https://www.thegoldensource.com/pca-and-the-term-structure/#:~:text=The%20purpose%20of%20PCA%20is,14%20orthogonal%20lines%20using%20eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b49cb-3610-4013-9627-2d8351cc6de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
